{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "149d147c-3134-4cf5-a5b4-0b895386de16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PatientId</th>\n",
       "      <th>AppointmentID</th>\n",
       "      <th>Gender</th>\n",
       "      <th>ScheduledDay</th>\n",
       "      <th>AppointmentDay</th>\n",
       "      <th>Age</th>\n",
       "      <th>Neighbourhood</th>\n",
       "      <th>Scholarship</th>\n",
       "      <th>Hipertension</th>\n",
       "      <th>Diabetes</th>\n",
       "      <th>Alcoholism</th>\n",
       "      <th>Handcap</th>\n",
       "      <th>SMS_received</th>\n",
       "      <th>No-show</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.987250e+13</td>\n",
       "      <td>5642903</td>\n",
       "      <td>F</td>\n",
       "      <td>2016-04-29T18:38:08Z</td>\n",
       "      <td>2016-04-29T00:00:00Z</td>\n",
       "      <td>62</td>\n",
       "      <td>JARDIM DA PENHA</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.589978e+14</td>\n",
       "      <td>5642503</td>\n",
       "      <td>M</td>\n",
       "      <td>2016-04-29T16:08:27Z</td>\n",
       "      <td>2016-04-29T00:00:00Z</td>\n",
       "      <td>56</td>\n",
       "      <td>JARDIM DA PENHA</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.262962e+12</td>\n",
       "      <td>5642549</td>\n",
       "      <td>F</td>\n",
       "      <td>2016-04-29T16:19:04Z</td>\n",
       "      <td>2016-04-29T00:00:00Z</td>\n",
       "      <td>62</td>\n",
       "      <td>MATA DA PRAIA</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8.679512e+11</td>\n",
       "      <td>5642828</td>\n",
       "      <td>F</td>\n",
       "      <td>2016-04-29T17:29:31Z</td>\n",
       "      <td>2016-04-29T00:00:00Z</td>\n",
       "      <td>8</td>\n",
       "      <td>PONTAL DE CAMBURI</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8.841186e+12</td>\n",
       "      <td>5642494</td>\n",
       "      <td>F</td>\n",
       "      <td>2016-04-29T16:07:23Z</td>\n",
       "      <td>2016-04-29T00:00:00Z</td>\n",
       "      <td>56</td>\n",
       "      <td>JARDIM DA PENHA</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      PatientId  AppointmentID Gender          ScheduledDay  \\\n",
       "0  2.987250e+13        5642903      F  2016-04-29T18:38:08Z   \n",
       "1  5.589978e+14        5642503      M  2016-04-29T16:08:27Z   \n",
       "2  4.262962e+12        5642549      F  2016-04-29T16:19:04Z   \n",
       "3  8.679512e+11        5642828      F  2016-04-29T17:29:31Z   \n",
       "4  8.841186e+12        5642494      F  2016-04-29T16:07:23Z   \n",
       "\n",
       "         AppointmentDay  Age      Neighbourhood  Scholarship  Hipertension  \\\n",
       "0  2016-04-29T00:00:00Z   62    JARDIM DA PENHA            0             1   \n",
       "1  2016-04-29T00:00:00Z   56    JARDIM DA PENHA            0             0   \n",
       "2  2016-04-29T00:00:00Z   62      MATA DA PRAIA            0             0   \n",
       "3  2016-04-29T00:00:00Z    8  PONTAL DE CAMBURI            0             0   \n",
       "4  2016-04-29T00:00:00Z   56    JARDIM DA PENHA            0             1   \n",
       "\n",
       "   Diabetes  Alcoholism  Handcap  SMS_received No-show  \n",
       "0         0           0        0             0      No  \n",
       "1         0           0        0             0      No  \n",
       "2         0           0        0             0      No  \n",
       "3         0           0        0             0      No  \n",
       "4         1           0        0             0      No  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv(\"KaggleV2-May-2016.csv\")\n",
    "\n",
    "# Preview the data\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "537a682f-5983-4535-9d1c-08a3f083248f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 110527 entries, 0 to 110526\n",
      "Data columns (total 14 columns):\n",
      " #   Column          Non-Null Count   Dtype  \n",
      "---  ------          --------------   -----  \n",
      " 0   PatientId       110527 non-null  float64\n",
      " 1   AppointmentID   110527 non-null  int64  \n",
      " 2   Gender          110527 non-null  object \n",
      " 3   ScheduledDay    110527 non-null  object \n",
      " 4   AppointmentDay  110527 non-null  object \n",
      " 5   Age             110527 non-null  int64  \n",
      " 6   Neighbourhood   110527 non-null  object \n",
      " 7   Scholarship     110527 non-null  int64  \n",
      " 8   Hipertension    110527 non-null  int64  \n",
      " 9   Diabetes        110527 non-null  int64  \n",
      " 10  Alcoholism      110527 non-null  int64  \n",
      " 11  Handcap         110527 non-null  int64  \n",
      " 12  SMS_received    110527 non-null  int64  \n",
      " 13  No-show         110527 non-null  object \n",
      "dtypes: float64(1), int64(8), object(5)\n",
      "memory usage: 11.8+ MB\n",
      "\n",
      "Missing values:\n",
      " PatientId         0\n",
      "AppointmentID     0\n",
      "Gender            0\n",
      "ScheduledDay      0\n",
      "AppointmentDay    0\n",
      "Age               0\n",
      "Neighbourhood     0\n",
      "Scholarship       0\n",
      "Hipertension      0\n",
      "Diabetes          0\n",
      "Alcoholism        0\n",
      "Handcap           0\n",
      "SMS_received      0\n",
      "No-show           0\n",
      "dtype: int64\n",
      "\n",
      "Target column unique values:\n",
      " No-show\n",
      "No     88208\n",
      "Yes    22319\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check dataset info\n",
    "df.info()\n",
    "\n",
    "# Check missing values\n",
    "print(\"\\nMissing values:\\n\", df.isnull().sum())\n",
    "\n",
    "# Check unique values in target column\n",
    "print(\"\\nTarget column unique values:\\n\", df['No-show'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8985afc4-bdbf-4d0a-b921-7f1969088490",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "25/07/14 19:41:56 WARN Utils: Your hostname, Taukirs-MacBook-Air-779.local, resolves to a loopback address: 127.0.0.1; using 192.168.0.98 instead (on interface en0)\n",
      "25/07/14 19:41:56 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/07/14 19:41:56 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------+------+-------------------+-------------------+---+-----------------+-----------+------------+--------+----------+-------+------------+-------+\n",
      "|          PatientId|AppointmentID|Gender|       ScheduledDay|     AppointmentDay|Age|    Neighbourhood|Scholarship|Hipertension|Diabetes|Alcoholism|Handcap|SMS_received|No-show|\n",
      "+-------------------+-------------+------+-------------------+-------------------+---+-----------------+-----------+------------+--------+----------+-------+------------+-------+\n",
      "| 2.9872499824296E13|      5642903|     F|2016-04-30 02:38:08|2016-04-29 08:00:00| 62|  JARDIM DA PENHA|          0|           1|       0|         0|      0|           0|     No|\n",
      "|5.58997776694438E14|      5642503|     M|2016-04-30 00:08:27|2016-04-29 08:00:00| 56|  JARDIM DA PENHA|          0|           0|       0|         0|      0|           0|     No|\n",
      "|  4.262962299951E12|      5642549|     F|2016-04-30 00:19:04|2016-04-29 08:00:00| 62|    MATA DA PRAIA|          0|           0|       0|         0|      0|           0|     No|\n",
      "|   8.67951213174E11|      5642828|     F|2016-04-30 01:29:31|2016-04-29 08:00:00|  8|PONTAL DE CAMBURI|          0|           0|       0|         0|      0|           0|     No|\n",
      "|  8.841186448183E12|      5642494|     F|2016-04-30 00:07:23|2016-04-29 08:00:00| 56|  JARDIM DA PENHA|          0|           1|       1|         0|      0|           0|     No|\n",
      "+-------------------+-------------+------+-------------------+-------------------+---+-----------------+-----------+------------+--------+----------+-------+------------+-------+\n",
      "only showing top 5 rows\n",
      "root\n",
      " |-- PatientId: double (nullable = true)\n",
      " |-- AppointmentID: integer (nullable = true)\n",
      " |-- Gender: string (nullable = true)\n",
      " |-- ScheduledDay: timestamp (nullable = true)\n",
      " |-- AppointmentDay: timestamp (nullable = true)\n",
      " |-- Age: integer (nullable = true)\n",
      " |-- Neighbourhood: string (nullable = true)\n",
      " |-- Scholarship: integer (nullable = true)\n",
      " |-- Hipertension: integer (nullable = true)\n",
      " |-- Diabetes: integer (nullable = true)\n",
      " |-- Alcoholism: integer (nullable = true)\n",
      " |-- Handcap: integer (nullable = true)\n",
      " |-- SMS_received: integer (nullable = true)\n",
      " |-- No-show: string (nullable = true)\n",
      "\n",
      "Rows : 110527\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = (SparkSession.builder\n",
    "         .appName(\"Hospital-NoShow-Prediction\")\n",
    "         .getOrCreate())\n",
    "\n",
    "df = (spark.read\n",
    "             .option(\"header\", True)\n",
    "             .option(\"inferSchema\", True)\n",
    "             .csv(\"KaggleV2-May-2016.csv\"))\n",
    "\n",
    "df.show(5)\n",
    "df.printSchema()\n",
    "print(\"Rows :\", df.count())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "491e62c7-cc33-40e8-8822-9680d6442d92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+---+-----------------+-----------+------------+\n",
      "|label|Gender|Age|    Neighbourhood|Scholarship|SMS_received|\n",
      "+-----+------+---+-----------------+-----------+------------+\n",
      "|    0|     F| 62|  JARDIM DA PENHA|          0|           0|\n",
      "|    0|     M| 56|  JARDIM DA PENHA|          0|           0|\n",
      "|    0|     F| 62|    MATA DA PRAIA|          0|           0|\n",
      "|    0|     F|  8|PONTAL DE CAMBURI|          0|           0|\n",
      "|    0|     F| 56|  JARDIM DA PENHA|          0|           0|\n",
      "+-----+------+---+-----------------+-----------+------------+\n",
      "only showing top 5 rows\n",
      "Rows after cleaning: 110519\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Create 'label' column: 1 if No-show == \"Yes\", else 0\n",
    "df = (\n",
    "    df.withColumnRenamed(\"No-show\", \"NoShow\")\n",
    "      .withColumn(\"label\", F.when(F.col(\"NoShow\") == \"Yes\", 1).otherwise(0).cast(\"int\"))\n",
    ")\n",
    "\n",
    "# Drop unnecessary columns\n",
    "df = df.drop(\"PatientId\", \"AppointmentID\", \"NoShow\")\n",
    "\n",
    "# Filter out invalid age rows\n",
    "df = df.filter((F.col(\"Age\") >= 0) & (F.col(\"Age\") <= 100))\n",
    "\n",
    "# Preview cleaned data\n",
    "df.select(\"label\", \"Gender\", \"Age\", \"Neighbourhood\", \"Scholarship\", \"SMS_received\").show(5)\n",
    "print(\"Rows after cleaning:\", df.count())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "edee0350-fb77-44ce-9d10-cbc285e43765",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------+-----+\n",
      "|features                                  |label|\n",
      "+------------------------------------------+-----+\n",
      "|(88,[0,4,81,83],[1.0,1.0,62.0,1.0])       |0    |\n",
      "|(88,[4,81],[1.0,56.0])                    |0    |\n",
      "|(88,[0,50,81],[1.0,1.0,62.0])             |0    |\n",
      "|(88,[0,76,81],[1.0,1.0,8.0])              |0    |\n",
      "|(88,[0,4,81,83,84],[1.0,1.0,56.0,1.0,1.0])|0    |\n",
      "+------------------------------------------+-----+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# Categorical columns to index and encode\n",
    "categorical_cols = [\"Gender\", \"Neighbourhood\"]\n",
    "\n",
    "# Numeric columns to keep as-is\n",
    "numeric_cols = [\"Age\", \"Scholarship\", \"Hipertension\", \"Diabetes\", \"Alcoholism\", \"Handcap\", \"SMS_received\"]\n",
    "\n",
    "# Indexers and Encoders\n",
    "indexers = [StringIndexer(inputCol=col, outputCol=col + \"_Index\") for col in categorical_cols]\n",
    "encoders = [OneHotEncoder(inputCol=col + \"_Index\", outputCol=col + \"_Vec\") for col in categorical_cols]\n",
    "\n",
    "# Final feature columns\n",
    "feature_cols = [col + \"_Vec\" for col in categorical_cols] + numeric_cols\n",
    "\n",
    "# Vector assembler to combine all features into a single vector column\n",
    "assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n",
    "\n",
    "# Create the pipeline\n",
    "pipeline = Pipeline(stages=indexers + encoders + [assembler])\n",
    "\n",
    "# Fit and transform the data\n",
    "model = pipeline.fit(df)\n",
    "df_transformed = model.transform(df)\n",
    "\n",
    "# Show features and label\n",
    "df_transformed.select(\"features\", \"label\").show(5, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1d8716c4-1e3f-4e2f-826c-9248b25b61f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Gender',\n",
       " 'ScheduledDay',\n",
       " 'AppointmentDay',\n",
       " 'Age',\n",
       " 'Neighbourhood',\n",
       " 'Scholarship',\n",
       " 'Hipertension',\n",
       " 'Diabetes',\n",
       " 'Alcoholism',\n",
       " 'Handcap',\n",
       " 'SMS_received',\n",
       " 'label']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1ea611da-c4ca-4798-9ee2-098b46b8d695",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into training (80%) and testing (20%)\n",
    "train_data, test_data = df_transformed.randomSplit([0.8, 0.2], seed=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "818499f8-7ee9-492e-8dd7-38f3337cc00d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/07/14 19:44:09 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "25/07/14 19:44:11 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "# Initialize logistic regression model\n",
    "lr = LogisticRegression(featuresCol='features', labelCol='label')\n",
    "\n",
    "# Train the model\n",
    "lr_model = lr.fit(train_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "04622c42-1fc7-46c2-9614-a3303d65073f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "# Initialize logistic regression model\n",
    "lr = LogisticRegression(featuresCol='features', labelCol='label')\n",
    "\n",
    "# Train the model\n",
    "lr_model = lr.fit(train_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "25213c5c-5411-4ea9-9167-db2a7ec45fe3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------+-----+----------+----------------------------------------+\n",
      "|features                                   |label|prediction|probability                             |\n",
      "+-------------------------------------------+-----+----------+----------------------------------------+\n",
      "|(88,[0,24,81,83,87],[1.0,1.0,66.0,1.0,1.0])|0    |0.0       |[0.7385515994321306,0.26144840056786944]|\n",
      "|(88,[0,27,81,87],[1.0,1.0,39.0,1.0])       |0    |0.0       |[0.7412969647410792,0.2587030352589208] |\n",
      "|(88,[0,24,81,83],[1.0,1.0,82.0,1.0])       |0    |0.0       |[0.8568118507769966,0.14318814922300338]|\n",
      "|(88,[0,27,81],[1.0,1.0,78.0])              |1    |0.0       |[0.8744093395076928,0.12559066049230716]|\n",
      "|(88,[0,24,81,83,87],[1.0,1.0,56.0,1.0,1.0])|0    |0.0       |[0.7268708092198128,0.27312919078018716]|\n",
      "+-------------------------------------------+-----+----------+----------------------------------------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "# Make predictions on the test data\n",
    "predictions = lr_model.transform(test_data)\n",
    "\n",
    "# Show predictions\n",
    "predictions.select(\"features\", \"label\", \"prediction\", \"probability\").show(5, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "96ed17ed-5740-4618-be91-35ffbe930e2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.7959\n",
      "Precision: 0.6335\n",
      "Recall:    0.7959\n",
      "F1 Score:  0.7055\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "# Initialize evaluators\n",
    "accuracy_evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "precision_evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"weightedPrecision\")\n",
    "recall_evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"weightedRecall\")\n",
    "f1_evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"f1\")\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy = accuracy_evaluator.evaluate(predictions)\n",
    "precision = precision_evaluator.evaluate(predictions)\n",
    "recall = recall_evaluator.evaluate(predictions)\n",
    "f1_score = f1_evaluator.evaluate(predictions)\n",
    "\n",
    "# Print the results\n",
    "print(f\"Accuracy:  {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall:    {recall:.4f}\")\n",
    "print(f\"F1 Score:  {f1_score:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91d74395-ff2d-4437-b42c-ae546aaed5a6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (venv310)",
   "language": "python",
   "name": "venv310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
