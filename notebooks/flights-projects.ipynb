{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2541d7a7-c6a1-4471-bb64-43db698738ac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<div class = \"ansiout\">\n",
       "This module provides various utilities for users to interact with the rest of Databricks.\n",
       "  <h3></h3><b>credentials: DatabricksCredentialUtils</b> -> Utilities for interacting with credentials within notebooks<br /><b>data: DataUtils</b> -> Utilities for understanding and interacting with datasets (EXPERIMENTAL)<br /><b>fs: DbfsUtils</b> -> Manipulates the Databricks filesystem (DBFS) from the console<br /><b>jobs: JobsUtils</b> -> Utilities for leveraging jobs features<br /><b>library: LibraryUtils</b> -> Utilities for session isolated libraries<br /><b>meta: MetaUtils</b> -> Methods to hook into the compiler (EXPERIMENTAL)<br /><b>notebook: NotebookUtils</b> -> Utilities for the control flow of a notebook (EXPERIMENTAL)<br /><b>preview: Preview</b> -> Utilities under preview category<br /><b>secrets: SecretUtils</b> -> Provides utilities for leveraging secrets within notebooks<br /><b>widgets: WidgetsUtils</b> -> Methods to create and get bound value of input widgets inside notebooks<br /><br /></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dbutils.help()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c885aa2e-34c0-44d7-b593-544f1a3b34b3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<div class = \"ansiout\">/**<br /> * Copies a file or directory, possibly across FileSystems..<br /> * <br /> * Example: cp(\"/mnt/my-folder/a\", \"s3n://bucket/b\")<br /> * <br /> * @param from FileSystem URI of the source file or directory<br /> * @param to FileSystem URI of the destination file or directory<br /> * @param recurse if true, all files and directories will be recursively copied<br /> * @return true if all files were successfully copied<br /> */<br /><b>cp(from: java.lang.String, to: java.lang.String, recurse: boolean = false): boolean</b></div><br />"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dbutils.fs.help('cp')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b8fba0bb-5ee6-43e5-9c88-9374def71a41",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>path</th><th>name</th><th>size</th><th>modificationTime</th></tr></thead><tbody><tr><td>dbfs:/databricks-datasets/COVID/</td><td>COVID/</td><td>0</td><td>1752483312699</td></tr><tr><td>dbfs:/databricks-datasets/README.md</td><td>README.md</td><td>976</td><td>1596557781000</td></tr><tr><td>dbfs:/databricks-datasets/Rdatasets/</td><td>Rdatasets/</td><td>0</td><td>1752483312699</td></tr><tr><td>dbfs:/databricks-datasets/SPARK_README.md</td><td>SPARK_README.md</td><td>3359</td><td>1596557823000</td></tr><tr><td>dbfs:/databricks-datasets/adult/</td><td>adult/</td><td>0</td><td>1752483312699</td></tr><tr><td>dbfs:/databricks-datasets/airlines/</td><td>airlines/</td><td>0</td><td>1752483312699</td></tr><tr><td>dbfs:/databricks-datasets/amazon/</td><td>amazon/</td><td>0</td><td>1752483312699</td></tr><tr><td>dbfs:/databricks-datasets/asa/</td><td>asa/</td><td>0</td><td>1752483312699</td></tr><tr><td>dbfs:/databricks-datasets/atlas_higgs/</td><td>atlas_higgs/</td><td>0</td><td>1752483312699</td></tr><tr><td>dbfs:/databricks-datasets/bikeSharing/</td><td>bikeSharing/</td><td>0</td><td>1752483312699</td></tr><tr><td>dbfs:/databricks-datasets/cctvVideos/</td><td>cctvVideos/</td><td>0</td><td>1752483312699</td></tr><tr><td>dbfs:/databricks-datasets/credit-card-fraud/</td><td>credit-card-fraud/</td><td>0</td><td>1752483312699</td></tr><tr><td>dbfs:/databricks-datasets/cs100/</td><td>cs100/</td><td>0</td><td>1752483312699</td></tr><tr><td>dbfs:/databricks-datasets/cs110x/</td><td>cs110x/</td><td>0</td><td>1752483312699</td></tr><tr><td>dbfs:/databricks-datasets/cs190/</td><td>cs190/</td><td>0</td><td>1752483312699</td></tr><tr><td>dbfs:/databricks-datasets/data.gov/</td><td>data.gov/</td><td>0</td><td>1752483312699</td></tr><tr><td>dbfs:/databricks-datasets/definitive-guide/</td><td>definitive-guide/</td><td>0</td><td>1752483312699</td></tr><tr><td>dbfs:/databricks-datasets/delta-sharing/</td><td>delta-sharing/</td><td>0</td><td>1752483312699</td></tr><tr><td>dbfs:/databricks-datasets/flights/</td><td>flights/</td><td>0</td><td>1752483312699</td></tr><tr><td>dbfs:/databricks-datasets/flower_photos/</td><td>flower_photos/</td><td>0</td><td>1752483312699</td></tr><tr><td>dbfs:/databricks-datasets/flowers/</td><td>flowers/</td><td>0</td><td>1752483312699</td></tr><tr><td>dbfs:/databricks-datasets/genomics/</td><td>genomics/</td><td>0</td><td>1752483312699</td></tr><tr><td>dbfs:/databricks-datasets/hail/</td><td>hail/</td><td>0</td><td>1752483312699</td></tr><tr><td>dbfs:/databricks-datasets/identifying-campaign-effectiveness/</td><td>identifying-campaign-effectiveness/</td><td>0</td><td>1752483312699</td></tr><tr><td>dbfs:/databricks-datasets/iot/</td><td>iot/</td><td>0</td><td>1752483312699</td></tr><tr><td>dbfs:/databricks-datasets/iot-stream/</td><td>iot-stream/</td><td>0</td><td>1752483312699</td></tr><tr><td>dbfs:/databricks-datasets/learning-spark/</td><td>learning-spark/</td><td>0</td><td>1752483312699</td></tr><tr><td>dbfs:/databricks-datasets/learning-spark-v2/</td><td>learning-spark-v2/</td><td>0</td><td>1752483312699</td></tr><tr><td>dbfs:/databricks-datasets/lending-club-loan-stats/</td><td>lending-club-loan-stats/</td><td>0</td><td>1752483312699</td></tr><tr><td>dbfs:/databricks-datasets/med-images/</td><td>med-images/</td><td>0</td><td>1752483312699</td></tr><tr><td>dbfs:/databricks-datasets/media/</td><td>media/</td><td>0</td><td>1752483312699</td></tr><tr><td>dbfs:/databricks-datasets/mnist-digits/</td><td>mnist-digits/</td><td>0</td><td>1752483312699</td></tr><tr><td>dbfs:/databricks-datasets/news20.binary/</td><td>news20.binary/</td><td>0</td><td>1752483312699</td></tr><tr><td>dbfs:/databricks-datasets/nyctaxi/</td><td>nyctaxi/</td><td>0</td><td>1752483312699</td></tr><tr><td>dbfs:/databricks-datasets/nyctaxi-with-zipcodes/</td><td>nyctaxi-with-zipcodes/</td><td>0</td><td>1752483312699</td></tr><tr><td>dbfs:/databricks-datasets/online_retail/</td><td>online_retail/</td><td>0</td><td>1752483312699</td></tr><tr><td>dbfs:/databricks-datasets/overlap-join/</td><td>overlap-join/</td><td>0</td><td>1752483312699</td></tr><tr><td>dbfs:/databricks-datasets/power-plant/</td><td>power-plant/</td><td>0</td><td>1752483312699</td></tr><tr><td>dbfs:/databricks-datasets/retail-org/</td><td>retail-org/</td><td>0</td><td>1752483312699</td></tr><tr><td>dbfs:/databricks-datasets/rwe/</td><td>rwe/</td><td>0</td><td>1752483312699</td></tr><tr><td>dbfs:/databricks-datasets/sai-summit-2019-sf/</td><td>sai-summit-2019-sf/</td><td>0</td><td>1752483312699</td></tr><tr><td>dbfs:/databricks-datasets/sample_logs/</td><td>sample_logs/</td><td>0</td><td>1752483312699</td></tr><tr><td>dbfs:/databricks-datasets/samples/</td><td>samples/</td><td>0</td><td>1752483312699</td></tr><tr><td>dbfs:/databricks-datasets/sfo_customer_survey/</td><td>sfo_customer_survey/</td><td>0</td><td>1752483312699</td></tr><tr><td>dbfs:/databricks-datasets/sms_spam_collection/</td><td>sms_spam_collection/</td><td>0</td><td>1752483312699</td></tr><tr><td>dbfs:/databricks-datasets/songs/</td><td>songs/</td><td>0</td><td>1752483312699</td></tr><tr><td>dbfs:/databricks-datasets/structured-streaming/</td><td>structured-streaming/</td><td>0</td><td>1752483312699</td></tr><tr><td>dbfs:/databricks-datasets/timeseries/</td><td>timeseries/</td><td>0</td><td>1752483312699</td></tr><tr><td>dbfs:/databricks-datasets/tpch/</td><td>tpch/</td><td>0</td><td>1752483312699</td></tr><tr><td>dbfs:/databricks-datasets/travel_recommendations_realtime/</td><td>travel_recommendations_realtime/</td><td>0</td><td>1752483312699</td></tr><tr><td>dbfs:/databricks-datasets/warmup/</td><td>warmup/</td><td>0</td><td>1752483312699</td></tr><tr><td>dbfs:/databricks-datasets/weather/</td><td>weather/</td><td>0</td><td>1752483312699</td></tr><tr><td>dbfs:/databricks-datasets/wiki/</td><td>wiki/</td><td>0</td><td>1752483312699</td></tr><tr><td>dbfs:/databricks-datasets/wikipedia-datasets/</td><td>wikipedia-datasets/</td><td>0</td><td>1752483312699</td></tr><tr><td>dbfs:/databricks-datasets/wine-quality/</td><td>wine-quality/</td><td>0</td><td>1752483312699</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "dbfs:/databricks-datasets/COVID/",
         "COVID/",
         0,
         1752483312699
        ],
        [
         "dbfs:/databricks-datasets/README.md",
         "README.md",
         976,
         1596557781000
        ],
        [
         "dbfs:/databricks-datasets/Rdatasets/",
         "Rdatasets/",
         0,
         1752483312699
        ],
        [
         "dbfs:/databricks-datasets/SPARK_README.md",
         "SPARK_README.md",
         3359,
         1596557823000
        ],
        [
         "dbfs:/databricks-datasets/adult/",
         "adult/",
         0,
         1752483312699
        ],
        [
         "dbfs:/databricks-datasets/airlines/",
         "airlines/",
         0,
         1752483312699
        ],
        [
         "dbfs:/databricks-datasets/amazon/",
         "amazon/",
         0,
         1752483312699
        ],
        [
         "dbfs:/databricks-datasets/asa/",
         "asa/",
         0,
         1752483312699
        ],
        [
         "dbfs:/databricks-datasets/atlas_higgs/",
         "atlas_higgs/",
         0,
         1752483312699
        ],
        [
         "dbfs:/databricks-datasets/bikeSharing/",
         "bikeSharing/",
         0,
         1752483312699
        ],
        [
         "dbfs:/databricks-datasets/cctvVideos/",
         "cctvVideos/",
         0,
         1752483312699
        ],
        [
         "dbfs:/databricks-datasets/credit-card-fraud/",
         "credit-card-fraud/",
         0,
         1752483312699
        ],
        [
         "dbfs:/databricks-datasets/cs100/",
         "cs100/",
         0,
         1752483312699
        ],
        [
         "dbfs:/databricks-datasets/cs110x/",
         "cs110x/",
         0,
         1752483312699
        ],
        [
         "dbfs:/databricks-datasets/cs190/",
         "cs190/",
         0,
         1752483312699
        ],
        [
         "dbfs:/databricks-datasets/data.gov/",
         "data.gov/",
         0,
         1752483312699
        ],
        [
         "dbfs:/databricks-datasets/definitive-guide/",
         "definitive-guide/",
         0,
         1752483312699
        ],
        [
         "dbfs:/databricks-datasets/delta-sharing/",
         "delta-sharing/",
         0,
         1752483312699
        ],
        [
         "dbfs:/databricks-datasets/flights/",
         "flights/",
         0,
         1752483312699
        ],
        [
         "dbfs:/databricks-datasets/flower_photos/",
         "flower_photos/",
         0,
         1752483312699
        ],
        [
         "dbfs:/databricks-datasets/flowers/",
         "flowers/",
         0,
         1752483312699
        ],
        [
         "dbfs:/databricks-datasets/genomics/",
         "genomics/",
         0,
         1752483312699
        ],
        [
         "dbfs:/databricks-datasets/hail/",
         "hail/",
         0,
         1752483312699
        ],
        [
         "dbfs:/databricks-datasets/identifying-campaign-effectiveness/",
         "identifying-campaign-effectiveness/",
         0,
         1752483312699
        ],
        [
         "dbfs:/databricks-datasets/iot/",
         "iot/",
         0,
         1752483312699
        ],
        [
         "dbfs:/databricks-datasets/iot-stream/",
         "iot-stream/",
         0,
         1752483312699
        ],
        [
         "dbfs:/databricks-datasets/learning-spark/",
         "learning-spark/",
         0,
         1752483312699
        ],
        [
         "dbfs:/databricks-datasets/learning-spark-v2/",
         "learning-spark-v2/",
         0,
         1752483312699
        ],
        [
         "dbfs:/databricks-datasets/lending-club-loan-stats/",
         "lending-club-loan-stats/",
         0,
         1752483312699
        ],
        [
         "dbfs:/databricks-datasets/med-images/",
         "med-images/",
         0,
         1752483312699
        ],
        [
         "dbfs:/databricks-datasets/media/",
         "media/",
         0,
         1752483312699
        ],
        [
         "dbfs:/databricks-datasets/mnist-digits/",
         "mnist-digits/",
         0,
         1752483312699
        ],
        [
         "dbfs:/databricks-datasets/news20.binary/",
         "news20.binary/",
         0,
         1752483312699
        ],
        [
         "dbfs:/databricks-datasets/nyctaxi/",
         "nyctaxi/",
         0,
         1752483312699
        ],
        [
         "dbfs:/databricks-datasets/nyctaxi-with-zipcodes/",
         "nyctaxi-with-zipcodes/",
         0,
         1752483312699
        ],
        [
         "dbfs:/databricks-datasets/online_retail/",
         "online_retail/",
         0,
         1752483312699
        ],
        [
         "dbfs:/databricks-datasets/overlap-join/",
         "overlap-join/",
         0,
         1752483312699
        ],
        [
         "dbfs:/databricks-datasets/power-plant/",
         "power-plant/",
         0,
         1752483312699
        ],
        [
         "dbfs:/databricks-datasets/retail-org/",
         "retail-org/",
         0,
         1752483312699
        ],
        [
         "dbfs:/databricks-datasets/rwe/",
         "rwe/",
         0,
         1752483312699
        ],
        [
         "dbfs:/databricks-datasets/sai-summit-2019-sf/",
         "sai-summit-2019-sf/",
         0,
         1752483312699
        ],
        [
         "dbfs:/databricks-datasets/sample_logs/",
         "sample_logs/",
         0,
         1752483312699
        ],
        [
         "dbfs:/databricks-datasets/samples/",
         "samples/",
         0,
         1752483312699
        ],
        [
         "dbfs:/databricks-datasets/sfo_customer_survey/",
         "sfo_customer_survey/",
         0,
         1752483312699
        ],
        [
         "dbfs:/databricks-datasets/sms_spam_collection/",
         "sms_spam_collection/",
         0,
         1752483312699
        ],
        [
         "dbfs:/databricks-datasets/songs/",
         "songs/",
         0,
         1752483312699
        ],
        [
         "dbfs:/databricks-datasets/structured-streaming/",
         "structured-streaming/",
         0,
         1752483312699
        ],
        [
         "dbfs:/databricks-datasets/timeseries/",
         "timeseries/",
         0,
         1752483312699
        ],
        [
         "dbfs:/databricks-datasets/tpch/",
         "tpch/",
         0,
         1752483312699
        ],
        [
         "dbfs:/databricks-datasets/travel_recommendations_realtime/",
         "travel_recommendations_realtime/",
         0,
         1752483312699
        ],
        [
         "dbfs:/databricks-datasets/warmup/",
         "warmup/",
         0,
         1752483312699
        ],
        [
         "dbfs:/databricks-datasets/weather/",
         "weather/",
         0,
         1752483312699
        ],
        [
         "dbfs:/databricks-datasets/wiki/",
         "wiki/",
         0,
         1752483312699
        ],
        [
         "dbfs:/databricks-datasets/wikipedia-datasets/",
         "wikipedia-datasets/",
         0,
         1752483312699
        ],
        [
         "dbfs:/databricks-datasets/wine-quality/",
         "wine-quality/",
         0,
         1752483312699
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "path",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "size",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "modificationTime",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%fs ls dbfs:/databricks-datasets/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "47d1efa4-62a0-4adb-88f5-89dedeaa6d20",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.fs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dfbce8fd-9082-4741-b3c4-f6bdb16f8d1a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+--------+------+-----------+\n|   date|delay|distance|origin|destination|\n+-------+-----+--------+------+-----------+\n|1011245|    6|     602|   ABE|        ATL|\n|1020600|   -8|     369|   ABE|        DTW|\n|1021245|   -2|     602|   ABE|        ATL|\n|1020605|   -4|     602|   ABE|        ATL|\n|1031245|   -4|     602|   ABE|        ATL|\n+-------+-----+--------+------+-----------+\nonly showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.csv(\"dbfs:/databricks-datasets/flights/departuredelays.csv\", header=True, inferSchema=True)\n",
    "df.show(5)  # View the top 5 rows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "42bc0d73-4062-4009-9214-701a30596d71",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- date: integer (nullable = true)\n |-- delay: integer (nullable = true)\n |-- distance: integer (nullable = true)\n |-- origin: string (nullable = true)\n |-- destination: string (nullable = true)\n\n+-------+-----------------+------------------+-----------------+-------+-----------+\n|summary|             date|             delay|         distance| origin|destination|\n+-------+-----------------+------------------+-----------------+-------+-----------+\n|  count|          1391578|           1391578|          1391578|1391578|    1391578|\n|   mean|2180446.584000322|12.079802928761449|690.5508264718184|   NULL|       NULL|\n| stddev|838031.1536740944|38.807733749856446|513.6628153663218|   NULL|       NULL|\n|    min|          1010005|              -112|               21|    ABE|        ABE|\n|    max|          3312359|              1642|             4330|    YUM|        YUM|\n+-------+-----------------+------------------+-----------------+-------+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()     # See column types\n",
    "df.columns           # List all columns\n",
    "df.describe().show() # Summary stats like count, mean, stddev\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "152d136f-d52b-4c9d-a62e-75f05bdfd72e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- date: integer (nullable = true)\n |-- delay: integer (nullable = true)\n |-- distance: integer (nullable = true)\n |-- origin: string (nullable = true)\n |-- destination: string (nullable = true)\n\n+-------+-----------------+------------------+-----------------+-------+-----------+\n|summary|             date|             delay|         distance| origin|destination|\n+-------+-----------------+------------------+-----------------+-------+-----------+\n|  count|          1391578|           1391578|          1391578|1391578|    1391578|\n|   mean|2180446.584000322|12.079802928761449|690.5508264718184|   NULL|       NULL|\n| stddev|838031.1536740944|38.807733749856446|513.6628153663218|   NULL|       NULL|\n|    min|          1010005|              -112|               21|    ABE|        ABE|\n|    max|          3312359|              1642|             4330|    YUM|        YUM|\n+-------+-----------------+------------------+-----------------+-------+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()\n",
    "df.columns\n",
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7009d7ea-1322-4888-a524-77f01bd31f0c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n|origin|\n+------+\n|   AEX|\n|   BFL|\n|   BDL|\n|   BIL|\n|   AVL|\n|   BMI|\n|   ALB|\n|   ALO|\n|   AUS|\n|   AVP|\n|   BIS|\n|   AGS|\n|   BET|\n|   BNA|\n|   ANC|\n|   ABI|\n|   AZO|\n|   ATL|\n|   AMA|\n|   ACT|\n+------+\nonly showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "df.select(\"origin\").distinct().show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "289ef263-6496-46d0-a61f-eb54344728c0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+--------+------+-----------+\n| date|delay|distance|origin|destination|\n+-----+-----+--------+------+-----------+\n|false|false|   false| false|      false|\n|false|false|   false| false|      false|\n|false|false|   false| false|      false|\n|false|false|   false| false|      false|\n|false|false|   false| false|      false|\n+-----+-----+--------+------+-----------+\nonly showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Check for nulls\n",
    "df.select([col(c).isNull().alias(c) for c in df.columns]).show(5)\n",
    "\n",
    "# Optional: Filter out rows with missing or unrealistic data\n",
    "df = df.filter((df.delay.isNotNull()) & (df.delay >= 0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "98af748f-9c2f-4af5-9463-0b76ace4930f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+\n|origin|count|\n+------+-----+\n|   ATL|41828|\n|   ORD|33812|\n|   DEN|30760|\n|   DFW|28706|\n|   LAX|22684|\n|   IAH|21009|\n|   PHX|17555|\n|   LAS|16938|\n|   SFO|16552|\n|   MCO|14189|\n+------+-----+\nonly showing top 10 rows\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-5353244931155773>, line 5\u001B[0m\n",
       "\u001B[1;32m      2\u001B[0m df\u001B[38;5;241m.\u001B[39mfilter(df\u001B[38;5;241m.\u001B[39mdelay \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m)\u001B[38;5;241m.\u001B[39mgroupBy(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124morigin\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39mcount()\u001B[38;5;241m.\u001B[39morderBy(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcount\u001B[39m\u001B[38;5;124m\"\u001B[39m, ascending\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\u001B[38;5;241m.\u001B[39mshow(\u001B[38;5;241m10\u001B[39m)\n",
       "\u001B[1;32m      4\u001B[0m \u001B[38;5;66;03m# Average delay by destination\u001B[39;00m\n",
       "\u001B[0;32m----> 5\u001B[0m df\u001B[38;5;241m.\u001B[39mgroupBy(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdest\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39mavg(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdelay\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39morderBy(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mavg(delay)\u001B[39m\u001B[38;5;124m\"\u001B[39m, ascending\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\u001B[38;5;241m.\u001B[39mshow(\u001B[38;5;241m10\u001B[39m)\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/dataframe.py:536\u001B[0m, in \u001B[0;36mDataFrame.groupBy\u001B[0;34m(self, *cols)\u001B[0m\n",
       "\u001B[1;32m    534\u001B[0m     _cols\u001B[38;5;241m.\u001B[39mappend(c)\n",
       "\u001B[1;32m    535\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(c, \u001B[38;5;28mstr\u001B[39m):\n",
       "\u001B[0;32m--> 536\u001B[0m     _cols\u001B[38;5;241m.\u001B[39mappend(\u001B[38;5;28mself\u001B[39m[c])\n",
       "\u001B[1;32m    537\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(c, \u001B[38;5;28mint\u001B[39m) \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(c, \u001B[38;5;28mbool\u001B[39m):\n",
       "\u001B[1;32m    538\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m c \u001B[38;5;241m<\u001B[39m \u001B[38;5;241m1\u001B[39m:\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/dataframe.py:1861\u001B[0m, in \u001B[0;36mDataFrame.__getitem__\u001B[0;34m(self, item)\u001B[0m\n",
       "\u001B[1;32m   1858\u001B[0m             \u001B[38;5;66;03m# Try best to verify the column name with cached schema\u001B[39;00m\n",
       "\u001B[1;32m   1859\u001B[0m             \u001B[38;5;66;03m# If fails, fall back to the server side validation\u001B[39;00m\n",
       "\u001B[1;32m   1860\u001B[0m             \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m verify_col_name(item, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mschema):\n",
       "\u001B[0;32m-> 1861\u001B[0m                 \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mselect(item)\u001B[38;5;241m.\u001B[39misLocal()\n",
       "\u001B[1;32m   1863\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_col(item)\n",
       "\u001B[1;32m   1864\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(item, Column):\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/dataframe.py:2008\u001B[0m, in \u001B[0;36mDataFrame.isLocal\u001B[0;34m(self)\u001B[0m\n",
       "\u001B[1;32m   2005\u001B[0m \u001B[38;5;129m@functools\u001B[39m\u001B[38;5;241m.\u001B[39mcache\n",
       "\u001B[1;32m   2006\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21misLocal\u001B[39m(\u001B[38;5;28mself\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28mbool\u001B[39m:\n",
       "\u001B[1;32m   2007\u001B[0m     query \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_plan\u001B[38;5;241m.\u001B[39mto_proto(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_session\u001B[38;5;241m.\u001B[39mclient)\n",
       "\u001B[0;32m-> 2008\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_session\u001B[38;5;241m.\u001B[39mclient\u001B[38;5;241m.\u001B[39m_analyze(method\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mis_local\u001B[39m\u001B[38;5;124m\"\u001B[39m, plan\u001B[38;5;241m=\u001B[39mquery)\u001B[38;5;241m.\u001B[39mis_local\n",
       "\u001B[1;32m   2009\u001B[0m     \u001B[38;5;28;01massert\u001B[39;00m result \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n",
       "\u001B[1;32m   2010\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m result\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/client/core.py:1540\u001B[0m, in \u001B[0;36mSparkConnectClient._analyze\u001B[0;34m(self, method, **kwargs)\u001B[0m\n",
       "\u001B[1;32m   1538\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m SparkConnectException(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mInvalid state during retry exception handling.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
       "\u001B[1;32m   1539\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m error:\n",
       "\u001B[0;32m-> 1540\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_handle_error(error)\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/client/core.py:2047\u001B[0m, in \u001B[0;36mSparkConnectClient._handle_error\u001B[0;34m(self, error)\u001B[0m\n",
       "\u001B[1;32m   2045\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mthread_local\u001B[38;5;241m.\u001B[39minside_error_handling \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n",
       "\u001B[1;32m   2046\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(error, grpc\u001B[38;5;241m.\u001B[39mRpcError):\n",
       "\u001B[0;32m-> 2047\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_handle_rpc_error(error)\n",
       "\u001B[1;32m   2048\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(error, \u001B[38;5;167;01mValueError\u001B[39;00m):\n",
       "\u001B[1;32m   2049\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCannot invoke RPC\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mstr\u001B[39m(error) \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mclosed\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mstr\u001B[39m(error):\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/client/core.py:2149\u001B[0m, in \u001B[0;36mSparkConnectClient._handle_rpc_error\u001B[0;34m(self, rpc_error)\u001B[0m\n",
       "\u001B[1;32m   2134\u001B[0m                 \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m(\n",
       "\u001B[1;32m   2135\u001B[0m                     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mPython versions in the Spark Connect client and server are different. \u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
       "\u001B[1;32m   2136\u001B[0m                     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTo execute user-defined functions, client and server should have the \u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
       "\u001B[0;32m   (...)\u001B[0m\n",
       "\u001B[1;32m   2145\u001B[0m                     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhttps://docs.databricks.com/en/release-notes/serverless.html.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
       "\u001B[1;32m   2146\u001B[0m                 )\n",
       "\u001B[1;32m   2147\u001B[0m             \u001B[38;5;66;03m# END-EDGE\u001B[39;00m\n",
       "\u001B[0;32m-> 2149\u001B[0m             \u001B[38;5;28;01mraise\u001B[39;00m convert_exception(\n",
       "\u001B[1;32m   2150\u001B[0m                 info,\n",
       "\u001B[1;32m   2151\u001B[0m                 status\u001B[38;5;241m.\u001B[39mmessage,\n",
       "\u001B[1;32m   2152\u001B[0m                 \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_fetch_enriched_error(info),\n",
       "\u001B[1;32m   2153\u001B[0m                 \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_display_server_stack_trace(),\n",
       "\u001B[1;32m   2154\u001B[0m             ) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n",
       "\u001B[1;32m   2156\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m SparkConnectGrpcException(status\u001B[38;5;241m.\u001B[39mmessage) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n",
       "\u001B[1;32m   2157\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\n",
       "\u001B[0;31mAnalysisException\u001B[0m: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `dest` cannot be resolved. Did you mean one of the following? [`date`, `delay`, `distance`, `origin`, `destination`]. SQLSTATE: 42703;\n",
       "'Project ['dest]\n",
       "+- Filter (isnotnull(delay#10532) AND (delay#10532 >= 0))\n",
       "   +- Relation [date#10531,delay#10532,distance#10533,origin#10534,destination#10535] csv\n",
       "\n",
       "\n",
       "JVM stacktrace:\n",
       "org.apache.spark.sql.catalyst.ExtendedAnalysisException\n",
       "\tat org.apache.spark.sql.errors.QueryCompilationErrors$.unresolvedAttributeError(QueryCompilationErrors.scala:504)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.org$apache$spark$sql$catalyst$analysis$CheckAnalysis$$failUnresolvedAttribute(CheckAnalysis.scala:179)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$10(CheckAnalysis.scala:489)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$10$adapted(CheckAnalysis.scala:474)\n",
       "\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:303)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$9(CheckAnalysis.scala:474)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$9$adapted(CheckAnalysis.scala:474)\n",
       "\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n",
       "\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n",
       "\tat scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:38)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:474)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:307)\n",
       "\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:303)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:307)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:278)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:416)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1(CheckAnalysis.scala:263)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:250)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:250)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:416)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$resolveInFixedPoint$1(HybridAnalyzer.scala:254)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:216)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:254)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:96)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:131)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:87)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:478)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:425)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:478)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$3(QueryExecution.scala:294)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:532)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$5(QueryExecution.scala:668)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.withExecutionPhase(SQLExecution.scala:150)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$4(QueryExecution.scala:668)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1308)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:661)\n",
       "\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:658)\n",
       "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1450)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:658)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:288)\n",
       "\tat com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:80)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:287)\n",
       "\tat scala.util.Try$.apply(Try.scala:213)\n",
       "\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1684)\n",
       "\tat org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1745)\n",
       "\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:321)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:267)\n",
       "\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$1(Dataset.scala:108)\n",
       "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1450)\n",
       "\tat org.apache.spark.sql.SparkSession.$anonfun$withActiveAndFrameProfiler$1(SparkSession.scala:1457)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\tat org.apache.spark.sql.SparkSession.withActiveAndFrameProfiler(SparkSession.scala:1457)\n",
       "\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:106)\n",
       "\tat org.apache.spark.sql.connect.service.SparkConnectAnalyzeHandler.process(SparkConnectAnalyzeHandler.scala:204)\n",
       "\tat org.apache.spark.sql.connect.service.SparkConnectAnalyzeHandler.$anonfun$handle$3(SparkConnectAnalyzeHandler.scala:104)\n",
       "\tat org.apache.spark.sql.connect.service.SparkConnectAnalyzeHandler.$anonfun$handle$3$adapted(SparkConnectAnalyzeHandler.scala:66)\n",
       "\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$2(SessionHolder.scala:464)\n",
       "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1450)\n",
       "\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$1(SessionHolder.scala:464)\n",
       "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:97)\n",
       "\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:90)\n",
       "\tat org.apache.spark.util.Utils$.withContextClassLoader(Utils.scala:241)\n",
       "\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:89)\n",
       "\tat org.apache.spark.sql.connect.service.SessionHolder.withSession(SessionHolder.scala:463)\n",
       "\tat org.apache.spark.sql.connect.service.SparkConnectAnalyzeHandler.$anonfun$handle$1(SparkConnectAnalyzeHandler.scala:66)\n",
       "\tat org.apache.spark.sql.connect.service.SparkConnectAnalyzeHandler.$anonfun$handle$1$adapted(SparkConnectAnalyzeHandler.scala:51)\n",
       "\tat com.databricks.spark.connect.logging.rpc.SparkConnectRpcMetricsCollectorUtils$.collectMetrics(SparkConnectRpcMetricsCollector.scala:258)\n",
       "\tat org.apache.spark.sql.connect.service.SparkConnectAnalyzeHandler.handle(SparkConnectAnalyzeHandler.scala:50)\n",
       "\tat org.apache.spark.sql.connect.service.SparkConnectService.analyzePlan(SparkConnectService.scala:109)\n",
       "\tat org.apache.spark.connect.proto.SparkConnectServiceGrpc$MethodHandlers.invoke(SparkConnectServiceGrpc.java:801)\n",
       "\tat grpc_shaded.io.grpc.stub.ServerCalls$UnaryServerCallHandler$UnaryServerCallListener.onHalfClose(ServerCalls.java:182)\n",
       "\tat grpc_shaded.io.grpc.PartialForwardingServerCallListener.onHalfClose(PartialForwardingServerCallListener.java:35)\n",
       "\tat grpc_shaded.io.grpc.ForwardingServerCallListener.onHalfClose(ForwardingServerCallListener.java:23)\n",
       "\tat grpc_shaded.io.grpc.ForwardingServerCallListener$SimpleForwardingServerCallListener.onHalfClose(ForwardingServerCallListener.java:40)\n",
       "\tat com.databricks.spark.connect.service.AuthenticationInterceptor$AuthenticatedServerCallListener.$anonfun$onHalfClose$1(AuthenticationInterceptor.scala:381)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)\n",
       "\tat com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)\n",
       "\tat com.databricks.spark.connect.service.RequestContext.$anonfun$runWith$3(RequestContext.scala:337)\n",
       "\tat com.databricks.spark.connect.service.RequestContext$.com$databricks$spark$connect$service$RequestContext$$withLocalProperties(RequestContext.scala:537)\n",
       "\tat com.databricks.spark.connect.service.RequestContext.$anonfun$runWith$2(RequestContext.scala:337)\n",
       "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)\n",
       "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)\n",
       "\tat com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:30)\n",
       "\tat com.databricks.spark.util.UniverseAttributionContextWrapper.withValue(AttributionContextUtils.scala:242)\n",
       "\tat com.databricks.spark.connect.service.RequestContext.$anonfun$runWith$1(RequestContext.scala:336)\n",
       "\tat com.databricks.spark.connect.service.RequestContext.withContext(RequestContext.scala:349)\n",
       "\tat com.databricks.spark.connect.service.RequestContext.runWith(RequestContext.scala:329)\n",
       "\tat com.databricks.spark.connect.service.AuthenticationInterceptor$AuthenticatedServerCallListener.onHalfClose(AuthenticationInterceptor.scala:381)\n",
       "\tat grpc_shaded.io.grpc.PartialForwardingServerCallListener.onHalfClose(PartialForwardingServerCallListener.java:35)\n",
       "\tat grpc_shaded.io.grpc.ForwardingServerCallListener.onHalfClose(ForwardingServerCallListener.java:23)\n",
       "\tat grpc_shaded.io.grpc.ForwardingServerCallListener$SimpleForwardingServerCallListener.onHalfClose(ForwardingServerCallListener.java:40)\n",
       "\tat grpc_shaded.io.grpc.internal.ServerCallImpl$ServerStreamListenerImpl.halfClosed(ServerCallImpl.java:351)\n",
       "\tat grpc_shaded.io.grpc.internal.ServerImpl$JumpToApplicationThreadServerStreamListener$1HalfClosed.runInContext(ServerImpl.java:861)\n",
       "\tat grpc_shaded.io.grpc.internal.ContextRunnable.run(ContextRunnable.java:37)\n",
       "\tat grpc_shaded.io.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:133)\n",
       "\tat org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.$anonfun$run$1(SparkThreadLocalForwardingThreadPoolExecutor.scala:161)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat com.databricks.sql.transaction.tahoe.mst.MSTThreadHelper$.runWithMstTxnId(MSTThreadHelper.scala:55)\n",
       "\tat org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.$anonfun$runWithCaptured$5(SparkThreadLocalForwardingThreadPoolExecutor.scala:116)\n",
       "\tat com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)\n",
       "\tat org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.$anonfun$runWithCaptured$4(SparkThreadLocalForwardingThreadPoolExecutor.scala:115)\n",
       "\tat com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)\n",
       "\tat org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.runWithCaptured(SparkThreadLocalForwardingThreadPoolExecutor.scala:114)\n",
       "\tat org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.runWithCaptured$(SparkThreadLocalForwardingThreadPoolExecutor.scala:91)\n",
       "\tat org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.runWithCaptured(SparkThreadLocalForwardingThreadPoolExecutor.scala:158)\n",
       "\tat org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.run(SparkThreadLocalForwardingThreadPoolExecutor.scala:161)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
       "\tat java.lang.Thread.run(Thread.java:840)"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": {
        "ename": "AnalysisException",
        "evalue": "[UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `dest` cannot be resolved. Did you mean one of the following? [`date`, `delay`, `distance`, `origin`, `destination`]. SQLSTATE: 42703;\n'Project ['dest]\n+- Filter (isnotnull(delay#10532) AND (delay#10532 >= 0))\n   +- Relation [date#10531,delay#10532,distance#10533,origin#10534,destination#10535] csv\n\n\nJVM stacktrace:\norg.apache.spark.sql.catalyst.ExtendedAnalysisException\n\tat org.apache.spark.sql.errors.QueryCompilationErrors$.unresolvedAttributeError(QueryCompilationErrors.scala:504)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.org$apache$spark$sql$catalyst$analysis$CheckAnalysis$$failUnresolvedAttribute(CheckAnalysis.scala:179)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$10(CheckAnalysis.scala:489)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$10$adapted(CheckAnalysis.scala:474)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:303)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$9(CheckAnalysis.scala:474)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$9$adapted(CheckAnalysis.scala:474)\n\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n\tat scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:38)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:474)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:307)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:303)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:307)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:278)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:416)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1(CheckAnalysis.scala:263)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:250)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:250)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:416)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$resolveInFixedPoint$1(HybridAnalyzer.scala:254)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:216)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:254)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:96)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:131)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:87)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:478)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:425)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:478)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$3(QueryExecution.scala:294)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:532)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$5(QueryExecution.scala:668)\n\tat org.apache.spark.sql.execution.SQLExecution$.withExecutionPhase(SQLExecution.scala:150)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$4(QueryExecution.scala:668)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1308)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:661)\n\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:658)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1450)\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:658)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:288)\n\tat com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:80)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:287)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1684)\n\tat org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1745)\n\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:321)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:267)\n\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$1(Dataset.scala:108)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1450)\n\tat org.apache.spark.sql.SparkSession.$anonfun$withActiveAndFrameProfiler$1(SparkSession.scala:1457)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat org.apache.spark.sql.SparkSession.withActiveAndFrameProfiler(SparkSession.scala:1457)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:106)\n\tat org.apache.spark.sql.connect.service.SparkConnectAnalyzeHandler.process(SparkConnectAnalyzeHandler.scala:204)\n\tat org.apache.spark.sql.connect.service.SparkConnectAnalyzeHandler.$anonfun$handle$3(SparkConnectAnalyzeHandler.scala:104)\n\tat org.apache.spark.sql.connect.service.SparkConnectAnalyzeHandler.$anonfun$handle$3$adapted(SparkConnectAnalyzeHandler.scala:66)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$2(SessionHolder.scala:464)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1450)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$1(SessionHolder.scala:464)\n\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:97)\n\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:90)\n\tat org.apache.spark.util.Utils$.withContextClassLoader(Utils.scala:241)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:89)\n\tat org.apache.spark.sql.connect.service.SessionHolder.withSession(SessionHolder.scala:463)\n\tat org.apache.spark.sql.connect.service.SparkConnectAnalyzeHandler.$anonfun$handle$1(SparkConnectAnalyzeHandler.scala:66)\n\tat org.apache.spark.sql.connect.service.SparkConnectAnalyzeHandler.$anonfun$handle$1$adapted(SparkConnectAnalyzeHandler.scala:51)\n\tat com.databricks.spark.connect.logging.rpc.SparkConnectRpcMetricsCollectorUtils$.collectMetrics(SparkConnectRpcMetricsCollector.scala:258)\n\tat org.apache.spark.sql.connect.service.SparkConnectAnalyzeHandler.handle(SparkConnectAnalyzeHandler.scala:50)\n\tat org.apache.spark.sql.connect.service.SparkConnectService.analyzePlan(SparkConnectService.scala:109)\n\tat org.apache.spark.connect.proto.SparkConnectServiceGrpc$MethodHandlers.invoke(SparkConnectServiceGrpc.java:801)\n\tat grpc_shaded.io.grpc.stub.ServerCalls$UnaryServerCallHandler$UnaryServerCallListener.onHalfClose(ServerCalls.java:182)\n\tat grpc_shaded.io.grpc.PartialForwardingServerCallListener.onHalfClose(PartialForwardingServerCallListener.java:35)\n\tat grpc_shaded.io.grpc.ForwardingServerCallListener.onHalfClose(ForwardingServerCallListener.java:23)\n\tat grpc_shaded.io.grpc.ForwardingServerCallListener$SimpleForwardingServerCallListener.onHalfClose(ForwardingServerCallListener.java:40)\n\tat com.databricks.spark.connect.service.AuthenticationInterceptor$AuthenticatedServerCallListener.$anonfun$onHalfClose$1(AuthenticationInterceptor.scala:381)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)\n\tat com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)\n\tat com.databricks.spark.connect.service.RequestContext.$anonfun$runWith$3(RequestContext.scala:337)\n\tat com.databricks.spark.connect.service.RequestContext$.com$databricks$spark$connect$service$RequestContext$$withLocalProperties(RequestContext.scala:537)\n\tat com.databricks.spark.connect.service.RequestContext.$anonfun$runWith$2(RequestContext.scala:337)\n\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)\n\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)\n\tat com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:30)\n\tat com.databricks.spark.util.UniverseAttributionContextWrapper.withValue(AttributionContextUtils.scala:242)\n\tat com.databricks.spark.connect.service.RequestContext.$anonfun$runWith$1(RequestContext.scala:336)\n\tat com.databricks.spark.connect.service.RequestContext.withContext(RequestContext.scala:349)\n\tat com.databricks.spark.connect.service.RequestContext.runWith(RequestContext.scala:329)\n\tat com.databricks.spark.connect.service.AuthenticationInterceptor$AuthenticatedServerCallListener.onHalfClose(AuthenticationInterceptor.scala:381)\n\tat grpc_shaded.io.grpc.PartialForwardingServerCallListener.onHalfClose(PartialForwardingServerCallListener.java:35)\n\tat grpc_shaded.io.grpc.ForwardingServerCallListener.onHalfClose(ForwardingServerCallListener.java:23)\n\tat grpc_shaded.io.grpc.ForwardingServerCallListener$SimpleForwardingServerCallListener.onHalfClose(ForwardingServerCallListener.java:40)\n\tat grpc_shaded.io.grpc.internal.ServerCallImpl$ServerStreamListenerImpl.halfClosed(ServerCallImpl.java:351)\n\tat grpc_shaded.io.grpc.internal.ServerImpl$JumpToApplicationThreadServerStreamListener$1HalfClosed.runInContext(ServerImpl.java:861)\n\tat grpc_shaded.io.grpc.internal.ContextRunnable.run(ContextRunnable.java:37)\n\tat grpc_shaded.io.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:133)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.$anonfun$run$1(SparkThreadLocalForwardingThreadPoolExecutor.scala:161)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.sql.transaction.tahoe.mst.MSTThreadHelper$.runWithMstTxnId(MSTThreadHelper.scala:55)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.$anonfun$runWithCaptured$5(SparkThreadLocalForwardingThreadPoolExecutor.scala:116)\n\tat com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.$anonfun$runWithCaptured$4(SparkThreadLocalForwardingThreadPoolExecutor.scala:115)\n\tat com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.runWithCaptured(SparkThreadLocalForwardingThreadPoolExecutor.scala:114)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.runWithCaptured$(SparkThreadLocalForwardingThreadPoolExecutor.scala:91)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.runWithCaptured(SparkThreadLocalForwardingThreadPoolExecutor.scala:158)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.run(SparkThreadLocalForwardingThreadPoolExecutor.scala:161)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.lang.Thread.run(Thread.java:840)"
       },
       "metadata": {
        "errorSummary": "[UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `dest` cannot be resolved. Did you mean one of the following? [`date`, `delay`, `distance`, `origin`, `destination`]. SQLSTATE: 42703"
       },
       "removedWidgets": [],
       "sqlProps": {
        "errorClass": "UNRESOLVED_COLUMN.WITH_SUGGESTION",
        "pysparkCallSite": "",
        "pysparkFragment": "",
        "pysparkSummary": null,
        "sqlState": "42703",
        "stackTrace": "org.apache.spark.sql.catalyst.ExtendedAnalysisException\n\tat org.apache.spark.sql.errors.QueryCompilationErrors$.unresolvedAttributeError(QueryCompilationErrors.scala:504)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.org$apache$spark$sql$catalyst$analysis$CheckAnalysis$$failUnresolvedAttribute(CheckAnalysis.scala:179)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$10(CheckAnalysis.scala:489)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$10$adapted(CheckAnalysis.scala:474)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:303)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$9(CheckAnalysis.scala:474)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$9$adapted(CheckAnalysis.scala:474)\n\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n\tat scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:38)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:474)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:307)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:303)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:307)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:278)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:416)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1(CheckAnalysis.scala:263)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:250)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:250)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:416)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$resolveInFixedPoint$1(HybridAnalyzer.scala:254)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:216)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:254)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:96)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:131)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:87)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:478)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:425)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:478)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$3(QueryExecution.scala:294)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:532)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$5(QueryExecution.scala:668)\n\tat org.apache.spark.sql.execution.SQLExecution$.withExecutionPhase(SQLExecution.scala:150)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$4(QueryExecution.scala:668)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1308)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:661)\n\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:658)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1450)\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:658)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:288)\n\tat com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:80)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:287)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1684)\n\tat org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1745)\n\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:321)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:267)\n\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$1(Dataset.scala:108)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1450)\n\tat org.apache.spark.sql.SparkSession.$anonfun$withActiveAndFrameProfiler$1(SparkSession.scala:1457)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat org.apache.spark.sql.SparkSession.withActiveAndFrameProfiler(SparkSession.scala:1457)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:106)\n\tat org.apache.spark.sql.connect.service.SparkConnectAnalyzeHandler.process(SparkConnectAnalyzeHandler.scala:204)\n\tat org.apache.spark.sql.connect.service.SparkConnectAnalyzeHandler.$anonfun$handle$3(SparkConnectAnalyzeHandler.scala:104)\n\tat org.apache.spark.sql.connect.service.SparkConnectAnalyzeHandler.$anonfun$handle$3$adapted(SparkConnectAnalyzeHandler.scala:66)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$2(SessionHolder.scala:464)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1450)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$1(SessionHolder.scala:464)\n\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:97)\n\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:90)\n\tat org.apache.spark.util.Utils$.withContextClassLoader(Utils.scala:241)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:89)\n\tat org.apache.spark.sql.connect.service.SessionHolder.withSession(SessionHolder.scala:463)\n\tat org.apache.spark.sql.connect.service.SparkConnectAnalyzeHandler.$anonfun$handle$1(SparkConnectAnalyzeHandler.scala:66)\n\tat org.apache.spark.sql.connect.service.SparkConnectAnalyzeHandler.$anonfun$handle$1$adapted(SparkConnectAnalyzeHandler.scala:51)\n\tat com.databricks.spark.connect.logging.rpc.SparkConnectRpcMetricsCollectorUtils$.collectMetrics(SparkConnectRpcMetricsCollector.scala:258)\n\tat org.apache.spark.sql.connect.service.SparkConnectAnalyzeHandler.handle(SparkConnectAnalyzeHandler.scala:50)\n\tat org.apache.spark.sql.connect.service.SparkConnectService.analyzePlan(SparkConnectService.scala:109)\n\tat org.apache.spark.connect.proto.SparkConnectServiceGrpc$MethodHandlers.invoke(SparkConnectServiceGrpc.java:801)\n\tat grpc_shaded.io.grpc.stub.ServerCalls$UnaryServerCallHandler$UnaryServerCallListener.onHalfClose(ServerCalls.java:182)\n\tat grpc_shaded.io.grpc.PartialForwardingServerCallListener.onHalfClose(PartialForwardingServerCallListener.java:35)\n\tat grpc_shaded.io.grpc.ForwardingServerCallListener.onHalfClose(ForwardingServerCallListener.java:23)\n\tat grpc_shaded.io.grpc.ForwardingServerCallListener$SimpleForwardingServerCallListener.onHalfClose(ForwardingServerCallListener.java:40)\n\tat com.databricks.spark.connect.service.AuthenticationInterceptor$AuthenticatedServerCallListener.$anonfun$onHalfClose$1(AuthenticationInterceptor.scala:381)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)\n\tat com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)\n\tat com.databricks.spark.connect.service.RequestContext.$anonfun$runWith$3(RequestContext.scala:337)\n\tat com.databricks.spark.connect.service.RequestContext$.com$databricks$spark$connect$service$RequestContext$$withLocalProperties(RequestContext.scala:537)\n\tat com.databricks.spark.connect.service.RequestContext.$anonfun$runWith$2(RequestContext.scala:337)\n\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)\n\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)\n\tat com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:30)\n\tat com.databricks.spark.util.UniverseAttributionContextWrapper.withValue(AttributionContextUtils.scala:242)\n\tat com.databricks.spark.connect.service.RequestContext.$anonfun$runWith$1(RequestContext.scala:336)\n\tat com.databricks.spark.connect.service.RequestContext.withContext(RequestContext.scala:349)\n\tat com.databricks.spark.connect.service.RequestContext.runWith(RequestContext.scala:329)\n\tat com.databricks.spark.connect.service.AuthenticationInterceptor$AuthenticatedServerCallListener.onHalfClose(AuthenticationInterceptor.scala:381)\n\tat grpc_shaded.io.grpc.PartialForwardingServerCallListener.onHalfClose(PartialForwardingServerCallListener.java:35)\n\tat grpc_shaded.io.grpc.ForwardingServerCallListener.onHalfClose(ForwardingServerCallListener.java:23)\n\tat grpc_shaded.io.grpc.ForwardingServerCallListener$SimpleForwardingServerCallListener.onHalfClose(ForwardingServerCallListener.java:40)\n\tat grpc_shaded.io.grpc.internal.ServerCallImpl$ServerStreamListenerImpl.halfClosed(ServerCallImpl.java:351)\n\tat grpc_shaded.io.grpc.internal.ServerImpl$JumpToApplicationThreadServerStreamListener$1HalfClosed.runInContext(ServerImpl.java:861)\n\tat grpc_shaded.io.grpc.internal.ContextRunnable.run(ContextRunnable.java:37)\n\tat grpc_shaded.io.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:133)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.$anonfun$run$1(SparkThreadLocalForwardingThreadPoolExecutor.scala:161)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.sql.transaction.tahoe.mst.MSTThreadHelper$.runWithMstTxnId(MSTThreadHelper.scala:55)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.$anonfun$runWithCaptured$5(SparkThreadLocalForwardingThreadPoolExecutor.scala:116)\n\tat com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.$anonfun$runWithCaptured$4(SparkThreadLocalForwardingThreadPoolExecutor.scala:115)\n\tat com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.runWithCaptured(SparkThreadLocalForwardingThreadPoolExecutor.scala:114)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.runWithCaptured$(SparkThreadLocalForwardingThreadPoolExecutor.scala:91)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.runWithCaptured(SparkThreadLocalForwardingThreadPoolExecutor.scala:158)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.run(SparkThreadLocalForwardingThreadPoolExecutor.scala:161)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.lang.Thread.run(Thread.java:840)",
        "startIndex": null,
        "stopIndex": null
       },
       "stackFrames": [
        "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
        "\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)",
        "File \u001B[0;32m<command-5353244931155773>, line 5\u001B[0m\n\u001B[1;32m      2\u001B[0m df\u001B[38;5;241m.\u001B[39mfilter(df\u001B[38;5;241m.\u001B[39mdelay \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m)\u001B[38;5;241m.\u001B[39mgroupBy(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124morigin\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39mcount()\u001B[38;5;241m.\u001B[39morderBy(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcount\u001B[39m\u001B[38;5;124m\"\u001B[39m, ascending\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\u001B[38;5;241m.\u001B[39mshow(\u001B[38;5;241m10\u001B[39m)\n\u001B[1;32m      4\u001B[0m \u001B[38;5;66;03m# Average delay by destination\u001B[39;00m\n\u001B[0;32m----> 5\u001B[0m df\u001B[38;5;241m.\u001B[39mgroupBy(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdest\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39mavg(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdelay\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39morderBy(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mavg(delay)\u001B[39m\u001B[38;5;124m\"\u001B[39m, ascending\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\u001B[38;5;241m.\u001B[39mshow(\u001B[38;5;241m10\u001B[39m)\n",
        "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/dataframe.py:536\u001B[0m, in \u001B[0;36mDataFrame.groupBy\u001B[0;34m(self, *cols)\u001B[0m\n\u001B[1;32m    534\u001B[0m     _cols\u001B[38;5;241m.\u001B[39mappend(c)\n\u001B[1;32m    535\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(c, \u001B[38;5;28mstr\u001B[39m):\n\u001B[0;32m--> 536\u001B[0m     _cols\u001B[38;5;241m.\u001B[39mappend(\u001B[38;5;28mself\u001B[39m[c])\n\u001B[1;32m    537\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(c, \u001B[38;5;28mint\u001B[39m) \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(c, \u001B[38;5;28mbool\u001B[39m):\n\u001B[1;32m    538\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m c \u001B[38;5;241m<\u001B[39m \u001B[38;5;241m1\u001B[39m:\n",
        "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/dataframe.py:1861\u001B[0m, in \u001B[0;36mDataFrame.__getitem__\u001B[0;34m(self, item)\u001B[0m\n\u001B[1;32m   1858\u001B[0m             \u001B[38;5;66;03m# Try best to verify the column name with cached schema\u001B[39;00m\n\u001B[1;32m   1859\u001B[0m             \u001B[38;5;66;03m# If fails, fall back to the server side validation\u001B[39;00m\n\u001B[1;32m   1860\u001B[0m             \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m verify_col_name(item, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mschema):\n\u001B[0;32m-> 1861\u001B[0m                 \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mselect(item)\u001B[38;5;241m.\u001B[39misLocal()\n\u001B[1;32m   1863\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_col(item)\n\u001B[1;32m   1864\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(item, Column):\n",
        "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/dataframe.py:2008\u001B[0m, in \u001B[0;36mDataFrame.isLocal\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   2005\u001B[0m \u001B[38;5;129m@functools\u001B[39m\u001B[38;5;241m.\u001B[39mcache\n\u001B[1;32m   2006\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21misLocal\u001B[39m(\u001B[38;5;28mself\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28mbool\u001B[39m:\n\u001B[1;32m   2007\u001B[0m     query \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_plan\u001B[38;5;241m.\u001B[39mto_proto(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_session\u001B[38;5;241m.\u001B[39mclient)\n\u001B[0;32m-> 2008\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_session\u001B[38;5;241m.\u001B[39mclient\u001B[38;5;241m.\u001B[39m_analyze(method\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mis_local\u001B[39m\u001B[38;5;124m\"\u001B[39m, plan\u001B[38;5;241m=\u001B[39mquery)\u001B[38;5;241m.\u001B[39mis_local\n\u001B[1;32m   2009\u001B[0m     \u001B[38;5;28;01massert\u001B[39;00m result \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   2010\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m result\n",
        "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/client/core.py:1540\u001B[0m, in \u001B[0;36mSparkConnectClient._analyze\u001B[0;34m(self, method, **kwargs)\u001B[0m\n\u001B[1;32m   1538\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m SparkConnectException(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mInvalid state during retry exception handling.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m   1539\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m error:\n\u001B[0;32m-> 1540\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_handle_error(error)\n",
        "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/client/core.py:2047\u001B[0m, in \u001B[0;36mSparkConnectClient._handle_error\u001B[0;34m(self, error)\u001B[0m\n\u001B[1;32m   2045\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mthread_local\u001B[38;5;241m.\u001B[39minside_error_handling \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[1;32m   2046\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(error, grpc\u001B[38;5;241m.\u001B[39mRpcError):\n\u001B[0;32m-> 2047\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_handle_rpc_error(error)\n\u001B[1;32m   2048\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(error, \u001B[38;5;167;01mValueError\u001B[39;00m):\n\u001B[1;32m   2049\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCannot invoke RPC\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mstr\u001B[39m(error) \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mclosed\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mstr\u001B[39m(error):\n",
        "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/client/core.py:2149\u001B[0m, in \u001B[0;36mSparkConnectClient._handle_rpc_error\u001B[0;34m(self, rpc_error)\u001B[0m\n\u001B[1;32m   2134\u001B[0m                 \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m(\n\u001B[1;32m   2135\u001B[0m                     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mPython versions in the Spark Connect client and server are different. \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   2136\u001B[0m                     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTo execute user-defined functions, client and server should have the \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   2145\u001B[0m                     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhttps://docs.databricks.com/en/release-notes/serverless.html.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   2146\u001B[0m                 )\n\u001B[1;32m   2147\u001B[0m             \u001B[38;5;66;03m# END-EDGE\u001B[39;00m\n\u001B[0;32m-> 2149\u001B[0m             \u001B[38;5;28;01mraise\u001B[39;00m convert_exception(\n\u001B[1;32m   2150\u001B[0m                 info,\n\u001B[1;32m   2151\u001B[0m                 status\u001B[38;5;241m.\u001B[39mmessage,\n\u001B[1;32m   2152\u001B[0m                 \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_fetch_enriched_error(info),\n\u001B[1;32m   2153\u001B[0m                 \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_display_server_stack_trace(),\n\u001B[1;32m   2154\u001B[0m             ) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   2156\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m SparkConnectGrpcException(status\u001B[38;5;241m.\u001B[39mmessage) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   2157\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
        "\u001B[0;31mAnalysisException\u001B[0m: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `dest` cannot be resolved. Did you mean one of the following? [`date`, `delay`, `distance`, `origin`, `destination`]. SQLSTATE: 42703;\n'Project ['dest]\n+- Filter (isnotnull(delay#10532) AND (delay#10532 >= 0))\n   +- Relation [date#10531,delay#10532,distance#10533,origin#10534,destination#10535] csv\n\n\nJVM stacktrace:\norg.apache.spark.sql.catalyst.ExtendedAnalysisException\n\tat org.apache.spark.sql.errors.QueryCompilationErrors$.unresolvedAttributeError(QueryCompilationErrors.scala:504)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.org$apache$spark$sql$catalyst$analysis$CheckAnalysis$$failUnresolvedAttribute(CheckAnalysis.scala:179)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$10(CheckAnalysis.scala:489)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$10$adapted(CheckAnalysis.scala:474)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:303)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$9(CheckAnalysis.scala:474)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$9$adapted(CheckAnalysis.scala:474)\n\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n\tat scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:38)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:474)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:307)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:303)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:307)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:278)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:416)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1(CheckAnalysis.scala:263)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:250)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:250)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:416)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$resolveInFixedPoint$1(HybridAnalyzer.scala:254)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:216)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:254)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:96)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:131)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:87)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:478)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:425)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:478)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$3(QueryExecution.scala:294)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:532)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$5(QueryExecution.scala:668)\n\tat org.apache.spark.sql.execution.SQLExecution$.withExecutionPhase(SQLExecution.scala:150)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$4(QueryExecution.scala:668)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1308)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:661)\n\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:658)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1450)\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:658)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:288)\n\tat com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:80)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:287)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1684)\n\tat org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1745)\n\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:321)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:267)\n\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$1(Dataset.scala:108)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1450)\n\tat org.apache.spark.sql.SparkSession.$anonfun$withActiveAndFrameProfiler$1(SparkSession.scala:1457)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat org.apache.spark.sql.SparkSession.withActiveAndFrameProfiler(SparkSession.scala:1457)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:106)\n\tat org.apache.spark.sql.connect.service.SparkConnectAnalyzeHandler.process(SparkConnectAnalyzeHandler.scala:204)\n\tat org.apache.spark.sql.connect.service.SparkConnectAnalyzeHandler.$anonfun$handle$3(SparkConnectAnalyzeHandler.scala:104)\n\tat org.apache.spark.sql.connect.service.SparkConnectAnalyzeHandler.$anonfun$handle$3$adapted(SparkConnectAnalyzeHandler.scala:66)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$2(SessionHolder.scala:464)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1450)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$1(SessionHolder.scala:464)\n\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:97)\n\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:90)\n\tat org.apache.spark.util.Utils$.withContextClassLoader(Utils.scala:241)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:89)\n\tat org.apache.spark.sql.connect.service.SessionHolder.withSession(SessionHolder.scala:463)\n\tat org.apache.spark.sql.connect.service.SparkConnectAnalyzeHandler.$anonfun$handle$1(SparkConnectAnalyzeHandler.scala:66)\n\tat org.apache.spark.sql.connect.service.SparkConnectAnalyzeHandler.$anonfun$handle$1$adapted(SparkConnectAnalyzeHandler.scala:51)\n\tat com.databricks.spark.connect.logging.rpc.SparkConnectRpcMetricsCollectorUtils$.collectMetrics(SparkConnectRpcMetricsCollector.scala:258)\n\tat org.apache.spark.sql.connect.service.SparkConnectAnalyzeHandler.handle(SparkConnectAnalyzeHandler.scala:50)\n\tat org.apache.spark.sql.connect.service.SparkConnectService.analyzePlan(SparkConnectService.scala:109)\n\tat org.apache.spark.connect.proto.SparkConnectServiceGrpc$MethodHandlers.invoke(SparkConnectServiceGrpc.java:801)\n\tat grpc_shaded.io.grpc.stub.ServerCalls$UnaryServerCallHandler$UnaryServerCallListener.onHalfClose(ServerCalls.java:182)\n\tat grpc_shaded.io.grpc.PartialForwardingServerCallListener.onHalfClose(PartialForwardingServerCallListener.java:35)\n\tat grpc_shaded.io.grpc.ForwardingServerCallListener.onHalfClose(ForwardingServerCallListener.java:23)\n\tat grpc_shaded.io.grpc.ForwardingServerCallListener$SimpleForwardingServerCallListener.onHalfClose(ForwardingServerCallListener.java:40)\n\tat com.databricks.spark.connect.service.AuthenticationInterceptor$AuthenticatedServerCallListener.$anonfun$onHalfClose$1(AuthenticationInterceptor.scala:381)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)\n\tat com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)\n\tat com.databricks.spark.connect.service.RequestContext.$anonfun$runWith$3(RequestContext.scala:337)\n\tat com.databricks.spark.connect.service.RequestContext$.com$databricks$spark$connect$service$RequestContext$$withLocalProperties(RequestContext.scala:537)\n\tat com.databricks.spark.connect.service.RequestContext.$anonfun$runWith$2(RequestContext.scala:337)\n\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)\n\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)\n\tat com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:30)\n\tat com.databricks.spark.util.UniverseAttributionContextWrapper.withValue(AttributionContextUtils.scala:242)\n\tat com.databricks.spark.connect.service.RequestContext.$anonfun$runWith$1(RequestContext.scala:336)\n\tat com.databricks.spark.connect.service.RequestContext.withContext(RequestContext.scala:349)\n\tat com.databricks.spark.connect.service.RequestContext.runWith(RequestContext.scala:329)\n\tat com.databricks.spark.connect.service.AuthenticationInterceptor$AuthenticatedServerCallListener.onHalfClose(AuthenticationInterceptor.scala:381)\n\tat grpc_shaded.io.grpc.PartialForwardingServerCallListener.onHalfClose(PartialForwardingServerCallListener.java:35)\n\tat grpc_shaded.io.grpc.ForwardingServerCallListener.onHalfClose(ForwardingServerCallListener.java:23)\n\tat grpc_shaded.io.grpc.ForwardingServerCallListener$SimpleForwardingServerCallListener.onHalfClose(ForwardingServerCallListener.java:40)\n\tat grpc_shaded.io.grpc.internal.ServerCallImpl$ServerStreamListenerImpl.halfClosed(ServerCallImpl.java:351)\n\tat grpc_shaded.io.grpc.internal.ServerImpl$JumpToApplicationThreadServerStreamListener$1HalfClosed.runInContext(ServerImpl.java:861)\n\tat grpc_shaded.io.grpc.internal.ContextRunnable.run(ContextRunnable.java:37)\n\tat grpc_shaded.io.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:133)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.$anonfun$run$1(SparkThreadLocalForwardingThreadPoolExecutor.scala:161)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.sql.transaction.tahoe.mst.MSTThreadHelper$.runWithMstTxnId(MSTThreadHelper.scala:55)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.$anonfun$runWithCaptured$5(SparkThreadLocalForwardingThreadPoolExecutor.scala:116)\n\tat com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.$anonfun$runWithCaptured$4(SparkThreadLocalForwardingThreadPoolExecutor.scala:115)\n\tat com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.runWithCaptured(SparkThreadLocalForwardingThreadPoolExecutor.scala:114)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.runWithCaptured$(SparkThreadLocalForwardingThreadPoolExecutor.scala:91)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.runWithCaptured(SparkThreadLocalForwardingThreadPoolExecutor.scala:158)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.run(SparkThreadLocalForwardingThreadPoolExecutor.scala:161)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.lang.Thread.run(Thread.java:840)"
       ],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Top 10 airports by number of delayed flights\n",
    "df.filter(df.delay > 0).groupBy(\"origin\").count().orderBy(\"count\", ascending=False).show(10)\n",
    "\n",
    "# Average delay by destination\n",
    "df.groupBy(\"dest\").avg(\"delay\").orderBy(\"avg(delay)\", ascending=False).show(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b6b2967d-f8f9-43e7-8011-46c7a879917a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pandas_df = df.limit(1000).toPandas()\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "pandas_df['delay'].hist(bins=30)\n",
    "plt.title(\"Flight Delays Distribution\")\n",
    "plt.xlabel(\"Delay (minutes)\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 8737873795251727,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "flights-projects",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}